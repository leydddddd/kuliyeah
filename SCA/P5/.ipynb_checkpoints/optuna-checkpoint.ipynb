{"cells":[{"cell_type":"markdown","id":"95550683","metadata":{"id":"95550683"},"source":["# Optuna untuk Hyperparameter Tuning XGBoost\n","\n","## Section 1: Penggunaan Simple Optuna\n","\n","**Apa itu Optuna?**\n","\n","Optuna adalah library Python yang dirancang khusus untuk melakukan **hyperparameter optimization** secara otomatis. Bayangkan Anda sedang memasak dan ingin menemukan kombinasi bumbu terbaik - Optuna seperti asisten yang secara cerdas mencoba berbagai kombinasi bumbu hingga menemukan rasa yang paling enak.\n","\n","**Bagaimana Optuna Bekerja?**\n","\n","1. **Trial**: Optuna menjalankan banyak \"trial\" atau percobaan. Setiap trial adalah satu kali training model dengan kombinasi hyperparameter yang berbeda.\n","\n","2. **Sampling Strategy**: Optuna menggunakan algoritma cerdas (seperti TPE - Tree-structured Parzen Estimator) untuk memilih hyperparameter yang akan dicoba selanjutnya. Ini jauh lebih efisien daripada random search atau grid search.\n","\n","3. **Objective Function**: Kita mendefinisikan fungsi objektif yang mengembalikan metrik yang ingin dioptimalkan (misalnya akurasi atau AUC).\n","\n","4. **Study**: Optuna mencatat semua trial dalam sebuah \"study\" dan terus mencari kombinasi hyperparameter terbaik.\n","\n","**Keuntungan Optuna:**\n","- Otomatis dan efisien\n","- Tidak perlu mendefinisikan semua kombinasi hyperparameter di muka\n","- Bisa berhenti lebih awal jika sudah menemukan hasil yang bagus\n","- Mudah digunakan dan terintegrasi dengan berbagai ML library"]},{"cell_type":"code","source":["!pip install optuna optuna-integration"],"metadata":{"id":"wYmBQJ7KMhGN"},"id":"wYmBQJ7KMhGN","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"7c5ecf57","metadata":{"id":"7c5ecf57"},"outputs":[],"source":["# Import library yang diperlukan\n","import numpy as np\n","import optuna\n","import sklearn.datasets\n","import sklearn.metrics\n","from sklearn.model_selection import train_test_split\n","import xgboost as xgb\n","\n","\n","def objective(trial):\n","    \"\"\"\n","    Fungsi objektif untuk Optuna.\n","\n","    Parameter 'trial' adalah objek yang diberikan Optuna untuk setiap percobaan.\n","    Trial ini berisi metode untuk 'menyarankan' nilai hyperparameter yang akan dicoba.\n","\n","    Fungsi ini harus mengembalikan nilai yang ingin kita optimalkan (dalam hal ini akurasi).\n","    Optuna akan mencoba memaksimalkan nilai ini.\n","    \"\"\"\n","\n","    # Load dataset breast cancer dari scikit-learn\n","    # Dataset ini cocok untuk binary classification\n","    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n","\n","    # Split data menjadi training dan validation set\n","    # random_state=42 memastikan split yang konsisten di setiap trial\n","    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25, random_state=42)\n","\n","    # Konversi data ke format DMatrix yang digunakan XGBoost\n","    # DMatrix adalah format data khusus XGBoost yang lebih efisien\n","    dtrain = xgb.DMatrix(train_x, label=train_y)\n","    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n","\n","    # Definisi hyperparameter untuk XGBoost\n","    # Optuna akan menyarankan nilai untuk hyperparameter ini di setiap trial\n","    param = {\n","        \"verbosity\": 0,  # Mengurangi output log XGBoost\n","        \"objective\": \"binary:logistic\",  # Untuk binary classification\n","        \"tree_method\": \"exact\",  # Metode untuk membangun tree\n","\n","        # trial.suggest_categorical: memilih dari daftar kategori\n","        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n","\n","        # trial.suggest_float: memilih nilai float dalam range tertentu\n","        # log=True berarti sampling dalam skala logaritmik (bagus untuk parameter seperti learning rate)\n","        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),  # L2 regularization\n","        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),   # L1 regularization\n","        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),  # Proporsi sampel yang digunakan\n","        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),  # Proporsi fitur yang digunakan\n","    }\n","\n","    # Hyperparameter khusus untuk booster tree-based (gbtree dan dart)\n","    if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n","        # trial.suggest_int: memilih nilai integer dalam range tertentu\n","        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)  # Kedalaman maksimum tree\n","        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)  # Minimum weight untuk child node\n","        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)  # Learning rate\n","        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)  # Minimum split loss\n","        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n","\n","    # Hyperparameter tambahan khusus untuk DART booster\n","    if param[\"booster\"] == \"dart\":\n","        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n","        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n","        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)  # Dropout rate\n","        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)  # Probabilitas skip dropout\n","\n","    # Training model XGBoost dengan hyperparameter yang disarankan\n","    bst = xgb.train(param, dtrain)\n","\n","    # Prediksi pada validation set\n","    preds = bst.predict(dvalid)\n","    pred_labels = np.rint(preds)  # Konversi probabilitas ke label (0 atau 1)\n","\n","    # Hitung akurasi sebagai metrik yang ingin dioptimalkan\n","    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n","\n","    # Return nilai yang ingin dimaksimalkan oleh Optuna\n","    return accuracy\n","\n","\n","# Buat study Optuna\n","# direction=\"maximize\" berarti kita ingin memaksimalkan nilai yang dikembalikan objective function\n","print(\"Memulai optimisasi hyperparameter dengan Optuna...\")\n","study = optuna.create_study(direction=\"maximize\")\n","\n","# Jalankan optimisasi\n","# n_trials=100: Optuna akan mencoba 100 kombinasi hyperparameter yang berbeda\n","# timeout=600: Batas waktu maksimal 600 detik (10 menit)\n","print(\"Menjalankan 100 trial optimisasi...\")\n","study.optimize(objective, n_trials=100, timeout=600)\n","\n","# Tampilkan hasil optimisasi\n","print(f\"\\nOptimisasi selesai!\")\n","print(f\"Jumlah trial yang diselesaikan: {len(study.trials)}\")\n","print(f\"\\nTrial terbaik:\")\n","trial = study.best_trial\n","print(f\"  Nilai akurasi terbaik: {trial.value:.4f}\")\n","print(f\"  Hyperparameter terbaik:\")\n","for k, v in trial.params.items():\n","    print(f\"    {k}: {v}\")"]},{"cell_type":"markdown","id":"e77eba8e","metadata":{"id":"e77eba8e"},"source":["# Section 2: Optuna dengan Pruning\n","\n","## Apa itu Pruning?\n","\n","**Pruning** adalah fitur canggih di Optuna yang memungkinkan untuk **menghentikan trial yang tidak menjanjikan lebih awal**. Ini seperti seorang pelatih yang bisa mengenali bahwa seorang atlet tidak akan menang dalam perlombaan, sehingga memutuskan untuk menghentikan partisipasinya dan menghemat energi untuk atlet lain yang lebih berpotensi.\n","\n","## Bagaimana Pruning Bekerja?\n","\n","1. **Early Stopping Cerdas**: Selama training model, Optuna memantau performa model secara berkala (misalnya setiap beberapa epoch).\n","\n","2. **Perbandingan dengan Trial Lain**: Pruner membandingkan performa trial saat ini dengan trial-trial sebelumnya pada titik yang sama dalam proses training.\n","\n","3. **Keputusan Pruning**: Jika trial saat ini menunjukkan performa yang jauh lebih buruk dibanding trial lain pada titik yang sama, maka trial tersebut dihentikan lebih awal.\n","\n","4. **Penghematan Waktu**: Dengan menghentikan trial yang tidak menjanjikan, kita bisa menggunakan waktu dan resource untuk mencoba kombinasi hyperparameter lain yang lebih berpotensi.\n","\n","## Jenis-jenis Pruner di Optuna:\n","\n","- **MedianPruner**: Menghentikan trial jika performanya di bawah median dari trial-trial sebelumnya\n","- **PercentilePruner**: Menghentikan trial jika performanya di bawah persentil tertentu\n","- **SuccessiveHalvingPruner**: Menggunakan algoritma successive halving\n","- **HyperbandPruner**: Kombinasi dari successive halving dengan multiple brackets\n","\n","## Keuntungan Pruning:\n","\n","✅ **Efisiensi Waktu**: Training yang lebih cepat karena trial buruk dihentikan lebih awal  \n","✅ **Resource Optimization**: Menghemat CPU/GPU untuk trial yang lebih menjanjikan  \n","✅ **Hasil Lebih Baik**: Dengan waktu yang sama, bisa mencoba lebih banyak kombinasi hyperparameter  \n","✅ **Otomatis**: Tidak perlu intervention manual untuk menghentikan trial yang buruk"]},{"cell_type":"code","execution_count":null,"id":"2322fa16","metadata":{"id":"2322fa16"},"outputs":[],"source":["# Import library yang sama seperti sebelumnya\n","import numpy as np\n","import optuna\n","import sklearn.datasets\n","import sklearn.metrics\n","from sklearn.model_selection import train_test_split\n","import xgboost as xgb\n","\n","def objective(trial):\n","    \"\"\"\n","    Objective function dengan implementasi pruning callback.\n","\n","    Perbedaan dengan versi sebelumnya:\n","    1. Menggunakan eval_metric untuk monitoring selama training\n","    2. Menggunakan XGBoostPruningCallback untuk pruning otomatis\n","    3. Training dengan evaluasi berkala (evals parameter)\n","    \"\"\"\n","\n","    # Load dan split data (sama seperti sebelumnya)\n","    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n","    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25, random_state=42)\n","\n","    # Konversi ke DMatrix\n","    dtrain = xgb.DMatrix(train_x, label=train_y)\n","    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n","\n","    # Hyperparameter configuration\n","    # eval_metric=\"auc\" penting untuk pruning - ini yang akan dimonitor\n","    param = {\n","        \"verbosity\": 0,\n","        \"objective\": \"binary:logistic\",\n","        \"eval_metric\": \"auc\",  # Metrik yang akan dimonitor untuk pruning\n","        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n","        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n","        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n","    }\n","\n","    # Hyperparameter untuk tree-based boosters\n","    if param[\"booster\"] in (\"gbtree\", \"dart\"):\n","        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n","        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n","        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n","        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n","\n","    # Hyperparameter tambahan untuk DART\n","    if param[\"booster\"] == \"dart\":\n","        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n","        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n","        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n","        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n","\n","    # KUNCI PRUNING: Buat XGBoostPruningCallback\n","    # Callback ini akan memantau \"validation-auc\" selama training\n","    # Jika performa buruk dibanding trial lain, akan menghentikan training lebih awal\n","    pruning_cb = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\")\n","\n","    # Training dengan monitoring dan callback pruning\n","    # evals: mendefinisikan dataset untuk evaluasi (validation set)\n","    # callbacks: list callback yang akan dijalankan, termasuk pruning callback\n","    bst = xgb.train(\n","        param,\n","        dtrain,\n","        evals=[(dvalid, \"validation\")],  # Dataset untuk evaluasi dan monitoring\n","        callbacks=[pruning_cb],  # Callback untuk pruning otomatis\n","        verbose_eval=False  # Mengurangi output selama training\n","    )\n","\n","    # Prediksi dan evaluasi final (sama seperti sebelumnya)\n","    preds = bst.predict(dvalid)\n","    pred_labels = np.rint(preds)\n","    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n","    return accuracy\n","\n","\n","# Buat study dengan pruner\n","# MedianPruner: menghentikan trial jika performanya di bawah median\n","# n_warmup_steps=5: tidak melakukan pruning pada 5 step pertama (biarkan model \"warming up\")\n","print(\"Memulai optimisasi dengan fitur pruning...\")\n","study = optuna.create_study(\n","    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n","    direction=\"maximize\",\n",")\n","\n","# Jalankan optimisasi (tanpa timeout kali ini, hanya berdasarkan jumlah trial)\n","print(\"Menjalankan optimisasi dengan pruning otomatis...\")\n","print(\"Trial yang tidak menjanjikan akan dihentikan lebih awal...\")\n","study.optimize(objective, n_trials=100)\n","\n","# Tampilkan hasil\n","best = study.best_trial\n","print(f\"\\nOptimisasi dengan pruning selesai!\")\n","print(f\"Jumlah trial yang diselesaikan: {len(study.trials)}\")\n","\n","# Hitung berapa trial yang di-prune\n","pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n","print(f\"Jumlah trial yang di-prune (dihentikan lebih awal): {len(pruned_trials)}\")\n","print(f\"Efisiensi pruning: {len(pruned_trials)/len(study.trials)*100:.1f}% trial dihentikan lebih awal\")\n","\n","print(f\"\\nTrial terbaik (dengan pruning):\")\n","print(f\"  Nilai akurasi terbaik: {best.value:.4f}\")\n","print(f\"  Hyperparameter terbaik:\")\n","for k, v in best.params.items():\n","    print(f\"    {k}: {v}\")"]},{"cell_type":"markdown","source":["Dengan pruning, kita bisa menghemat waktu karena trial yang buruk dihentikan lebih awal, sehingga bisa fokus pada hyperparameter yang menjanjikan!"],"metadata":{"id":"NXdqGu65NLeU"},"id":"NXdqGu65NLeU"}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}